{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. One client to rule them all\n",
    "\n",
    "The Dynamic Annotation Framework consists of a number of different services, each with a specific set of tasks that it can perform through REST endpoints. This module is designed to ease programmatic interaction with all of the various endpoints. Going forward, we also will be increasingly using authentication tokens for programmatic access to most if not all of the services. In order to collect a given server, dataset name, and user token together into a coherent package that can be used on multiple endpoints, we will use a FrameworkClient that can build appropriately configured clients for each of the specific services.\n",
    "\n",
    "The following examples cover each of the client subtypes that are associated with a single service. The ImageryClient, which is a more complex collection of tools, will be covered elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing a FrameworkClient\n",
    "Assuming that the services are on `www.dynamicannotationframework.com` and authentication tokens are either not being used or set up with default values (see next section), one needs only to specify the dataset name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annotationframeworkclient import FrameworkClient\n",
    "\n",
    "dataset_name = 'pinky100'\n",
    "client = FrameworkClient(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to confirm that this works, let's see if we can get the EM image source from the InfoService. If you get a reasonable looking path, everything is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image source is: https://storage.googleapis.com/neuroglancer/pinky100_v0/son_of_alignment_v15_rechunked\n"
     ]
    }
   ],
   "source": [
    "print(f\"The image source is: {client.info.image_source()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Authentication Service\n",
    "\n",
    "Going forward, we're going to need authentication tokens for programmatic access to our services. The AuthClient handles storing and loading your token or tokens and inserting it into requests in other clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the auth client from `client.auth`. Once you have saved a token, you probably won't interact with this client very often, however it has some convenient features for saving new tokens the first time. Let's see if you have a token already. Probably not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My current token is: fake_token_123\n"
     ]
    }
   ],
   "source": [
    "auth = client.auth\n",
    "print(f\"My current token is: {auth.token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a new token\n",
    "It is not yet possible to get a new token programmatically, but the function `get_new_token()` provides instructions for how to get and save it.\n",
    "\n",
    "By default, the token is saved to `~/.cloudvolume/secrets/chunkedgraph-secret.json` as a string under the key `token`. The following steps will save a token there.\n",
    "\n",
    "*Note: I am not sure where the auth server is being hosted right now, so we are going to use a fake token for documentation purposes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Tokens need to be acquired by hand. Please follow the following steps:\n",
      "                1) Go to: https://www.dynamicannotationframework.com/auth/refresh_token\n",
      "                2) Log in with your Google credentials and copy the token shown afterward.\n",
      "                3a) Save it to your computer with: client.auth.save_token(token=\"PASTE_YOUR_TOKEN_HERE\")\n",
      "                or\n",
      "                3b) Set it for the current session only with client.auth.token = \"PASTE_YOUR_TOKEN_HERE\"\n",
      "                Note: If you need to save or load multiple tokens, please read the documentation for details.\n",
      "                Warning! Creating a new token will invalidate the previous token!\n"
     ]
    }
   ],
   "source": [
    "auth.get_new_token(open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My token is now: fake_token_123\n"
     ]
    }
   ],
   "source": [
    "new_token = 'fake_token_123'\n",
    "auth.save_token(token=new_token, overwrite=True)\n",
    "print(f\"My token is now: {auth.token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading saved tokens\n",
    "Try opening `~/.cloudvolume/secrets/chunkedgraph-secret.json` to see what we just created.\n",
    "\n",
    "If we had wanted to use a different file or a different json key, we could have specified that in auth.save_token.\n",
    "\n",
    "Because we used the default values, this token is used automatically when we intialize a new FrameworkClient. If we wanted to use a different token file, token key, or even directly specify a token we could do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now my basic token is: fake_token_123\n",
      "A directly specified token is: another_fake_token_678\n"
     ]
    }
   ],
   "source": [
    "client = FrameworkClient(dataset_name)\n",
    "print(f\"Now my basic token is: {client.auth.token}\")\n",
    "\n",
    "client_direct = FrameworkClient(dataset_name, auth_token='another_fake_token_678')\n",
    "print(f\"A directly specified token is: {client_direct.auth.token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use a FrameworkClient, the AuthClient and its token will be automatically applied to any other services without further use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Info Service\n",
    "A dataset has a number of complex paths to various data sources that together comprise a dataset. Rather than hardcode these paths, the InfoService allows one to query the location of each data source. This is also convenient in case data sources change.\n",
    "\n",
    "An InfoClient is accessed at `client.info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<annotationframeworkclient.infoservice.InfoServiceClient at 0x10da163c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an info client for pinky100 on https://www.dynamicannotationframework.com\n"
     ]
    }
   ],
   "source": [
    "client = FrameworkClient(dataset_name)\n",
    "print(f\"This is an info client for {client.info.dataset_name} on {client.info.server_address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing dataset information\n",
    "All of the information accessible for the dataset can be seen as a dict using `get_dataset_info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis_database_ip': 'www.dynamicannotationframework.com',\n",
       " 'annotation_engine_endpoint': 'https://www.dynamicannotationframework.com/',\n",
       " 'flat_segmentation_source': 'https://storage.googleapis.com/neuroglancer/pinky100_v0/seg/lost_no-random/bbox1_0',\n",
       " 'graphene_source': 'graphene://https://www.dynamicannotationframework.com/segmentation/1.0/pinky100_sv16',\n",
       " 'id': 1,\n",
       " 'image_source': 'https://storage.googleapis.com/neuroglancer/pinky100_v0/son_of_alignment_v15_rechunked',\n",
       " 'name': 'pinky100',\n",
       " 'pychunkedgraph_supervoxel_source': 'https://storage.googleapis.com/neuroglancer/nkem/pinky100_v0/ws/lost_no-random/bbox1_0',\n",
       " 'pychunkedgraph_viewer_source': 'https://www.dynamicannotationframework.com/segmentation/1.0/pinky100_sv16',\n",
       " 'pychunkgraph_endpoint': 'http://35.237.202.194/',\n",
       " 'pychunkgraph_segmentation_source': 'https://www.dynamicannotationframework.com/segmentation/1.0/pinky100_sv16',\n",
       " 'synapse_segmentation_source': 'https://storage.googleapis.com/neuroglancer/pinky100_v0/clefts/mip1_d2_1175k'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.info.get_dataset_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual entries can be found as well. Use tab autocomplete to see the various possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphene://https://www.dynamicannotationframework.com/segmentation/1.0/pinky100_sv16'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.info.graphene_source()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting formatting\n",
    "Because of the way neuroglancer looks up data versus cloudvolume, sometimes one needs to convert between `gs://` style paths to `https://storage.googleapis.com/` stype paths. All of the path sources in the info client accept a `format_for` argument that can handle this, and correctly adapts to graphene vs precomputed data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With gs-style: precomputed://gs://neuroglancer/pinky100_v0/son_of_alignment_v15_rechunked\n",
      "With https-style: precomputed://https://storage.googleapis.com/neuroglancer/pinky100_v0/son_of_alignment_v15_rechunked\n"
     ]
    }
   ],
   "source": [
    "neuroglancer_style_source = client.info.image_source(format_for='neuroglancer')\n",
    "print(f\"With gs-style: { neuroglancer_style_source }\")\n",
    "\n",
    "cloudvolume_style_source = client.info.image_source(format_for='cloudvolume')\n",
    "print(f\"With https-style: { cloudvolume_style_source }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. JSON Service\n",
    "\n",
    "We store the JSON description of a Neuroglancer state in a simple database at the JSON Service. This is a convenient way to build states to distribute to people, or pull states to parse work by individuals. The JSON Client is at `client.state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<annotationframeworkclient.jsonservice.JSONService at 0x10da16748>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving a state\n",
    "\n",
    "JSON states are found simply by their ID, which you get when uploading a state. You can download a state with `get_state_json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'precomputed://gs://microns-seunglab/minnie_v4/alignment/fine/sergiy_multimodel_v1/vector_fixer30_faster_v01/image_stitch_multi_block_v1',\n",
       " 'type': 'image',\n",
       " 'blend': 'default',\n",
       " 'shaderControls': {},\n",
       " 'name': 'Minnie65 (MIP2+)'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_id = 4845531975188480\n",
    "example_state = client.state.get_state_json(example_id)\n",
    "example_state['layers'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading a state\n",
    "You can also upload states with `upload_state_json`. If you do this, the state id is returned by the function. Note that there is no easy way to query what you uploaded later, so be VERY CAREFUL with this state id if you wish to see it again.\n",
    "\n",
    "*Note: If you are working with a Neuroglancer Viewer object or similar, in order to upload, use viewer.state.to_json() to generate this representation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_state['layers'][0]['name'] = 'example_name'\n",
    "new_id = client.state.upload_state_json(example_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example_name'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state = client.state.get_state_json(new_id)\n",
    "test_state['layers'][0]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Neuroglancer URLs\n",
    "\n",
    "Neuroglancer can automatically look up a JSON state based on its ID if you pass the URL to it correctly. The function `build_neuroglancer_url` helps format these correctly. Note that you need to specify the base URL for the Neuroglancer deployment you wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neuromancer-seung-import.appspot.com/?json_url=https://www.dynamicannotationframework.com/nglstate/6553267771342848\n"
     ]
    }
   ],
   "source": [
    "url = client.state.build_neuroglancer_url(state_id=new_id, ngl_url='https://neuromancer-seung-import.appspot.com')\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ChunkedGraph\n",
    "\n",
    "The ChunkedGraph client allows one to interact with the ChunkedGraph, which stores and updates the supervoxel agglomeration graph. This is most often useful for looking up an object root id of a supervoxel or looking up supervoxels belonging to a root id. The ChunkedGraph client is at `client.chunkedgraph`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up a supervoxel\n",
    "Usually in Neuroglancer, one never notices supervoxel ids, but they are important for programmatic work. In order to look up the root id for a location in space, one needs to use the supervoxel segmentation to get the associated supervoxel id. The ChunkedGraph client makes this easy using the `get_root_ids` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([648518346349541252], dtype=uint64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_id = 104200755619042523\n",
    "client.chunkedgraph.get_root_id(supervoxel_id=sv_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as proofreading occurs, the root id that a supervoxel belongs to can change. By default, this function returns the current state, however one can also provide a UTC timestamp to get the root id at a particular moment in history. This can be useful for reproducible analysis. Note below that the root id for the same supervoxel is different than it is now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([648518346348172238], dtype=uint64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# I looked up the UTC POSIX timestamp from a day in early 2019. \n",
    "timestamp = datetime.datetime.utcfromtimestamp(1546595253)\n",
    "\n",
    "sv_id = 104200755619042523\n",
    "client.chunkedgraph.get_root_id(supervoxel_id=sv_id, timestamp=timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting supervoxels for a root id\n",
    "\n",
    "A root id is associated with a particular agglomeration of supervoxels, which can be found with the `get_leaves` method. A new root id is generated for every new change in the chunkedgraph, so time stamps do not apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 84777895599276224,  84777895599276227,  84777895599276228, ...,\n",
       "       104200755619042523, 104200755619042333, 104200755619042335],\n",
       "      dtype=uint64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_id = 648518346349541252\n",
    "client.chunkedgraph.get_leaves(root_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. AnnotationEngine\n",
    "\n",
    "The AnnotationClient is used to interact with the AnnotationEngine service to create tables from existing schema, upload new data, and download existing annotations. Note that annotations in the AnnotationEngine are not linked to any particular segmentation, and thus do not include any root ids. An annotation client is accessed with `client.annotation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get existing tables\n",
    "\n",
    "A list of the existing tables for the dataset can be found at with `get_tables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 124,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'ais_analysis_soma'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 312,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'ais_bounds'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 321,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'ais_bounds_v2'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 315,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'ais_bounds_v3'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 764,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'cell_type_ai_manual'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 298,\n",
       "  'schema_name': 'plastic_synapse',\n",
       "  'table_name': 'chandelier_plasticity'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 927,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'chc_input_valence'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 8623,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'er_points'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 512,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'functional_coregistration_lookup'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 512,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'functional_coregistration_raw'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 58,\n",
       "  'schema_name': 'extended_classical_cell_type',\n",
       "  'table_name': 'interneurons_putative_ai'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 0,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'interneurons_putative_ai_v0'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 0,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'is_chandelier'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 626,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'is_chandelier_manual'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 126,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'is_chandelier_v2'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 165,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'is_chandelier_v3'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 177,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'is_chandelier_v4'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 164,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'is_chandelier_v5'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 45,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'manual_ais'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 27,\n",
       "  'schema_name': 'microns_func_coreg',\n",
       "  'table_name': 'microglia_host'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 66,\n",
       "  'schema_name': 'glia_contact',\n",
       "  'table_name': 'microglia_soma_contacts'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 38,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'microglia_type'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 223,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'morph_basket_types_manual'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 203,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'morph_pancake_types_manual'},\n",
       " {'chunk_size': [512, 512, 128],\n",
       "  'max_annotation_id': 3260276,\n",
       "  'schema_name': 'synapse',\n",
       "  'table_name': 'pni_synapses'},\n",
       " {'chunk_size': [512, 512, 128],\n",
       "  'max_annotation_id': 3556643,\n",
       "  'schema_name': 'synapse',\n",
       "  'table_name': 'pni_synapses_i2'},\n",
       " {'chunk_size': [512, 512, 128],\n",
       "  'max_annotation_id': 3556643,\n",
       "  'schema_name': 'synapse',\n",
       "  'table_name': 'pni_synapses_i3'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 460,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'soma_valence'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 458,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'soma_valence_v2'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 551,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'svenmd_axons'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 319,\n",
       "  'schema_name': 'cell_type_local',\n",
       "  'table_name': 'svenmd_tracer_axons'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 628,\n",
       "  'schema_name': 'synapse',\n",
       "  'table_name': 'synapse_ai_manual'},\n",
       " {'chunk_size': [512, 512, 64],\n",
       "  'max_annotation_id': 626,\n",
       "  'schema_name': 'synapse',\n",
       "  'table_name': 'synapse_ai_manual_v2'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tables = client.annotation.get_tables()\n",
    "all_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each table has three main properties that can be useful to know:\n",
    "* `table_name` : The table name, used to refer to it when uploading or downloading annotations. This is also passed through to the table in the Materialized database.\n",
    "* `schema_name` : The name of the table's schema from EMAnnotationSchemas (see below).\n",
    "* `max_annotation_id` : An upper limit on the number of annotations already contained in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading annotations\n",
    "\n",
    "You can download the JSON representation of a data point through the `get_annotation` method. This can be useful if you need to look up information on unmaterialized data, or to see what a properly templated annotation looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'func_id': 0,\n",
       " 'oid': '100',\n",
       " 'pt': {'position': [111366, 62071, 1219]},\n",
       " 'type': 'microns_func_coreg'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = all_tables[0]['table_name'] # 'ais_analysis_soma'\n",
    "annotation_id = 100\n",
    "client.annotation.get_annotation(annotation_id=annotation_id, table_name=table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new table\n",
    "\n",
    "One can create a new table with a specified schema with the `create_table` method:\n",
    "\n",
    "```\n",
    "client.annotation.create_table(table_name='test_table',\n",
    "                               schema_name='microns_func_coreg')\n",
    "```\n",
    "\n",
    "Now, new data can be generated as a dict or list of dicts following the schema and uploaded with `post_annotation`.\n",
    "For example, a `microns_func_coreg` point needs to have:\n",
    "    * `type` set to `microns_func_coreg`\n",
    "    * `pt` set to a dict with `position` as a key and the xyz location as a value.\n",
    "    * `func_id` set to an integer.\n",
    "    \n",
    "The following will create a new annotation and upload it to the service: \n",
    "```\n",
    "new_data = {'type': 'microns_func_coreg',\n",
    "            'pt': {'position': [1,2,3]},\n",
    "            'func_id': 0}\n",
    "            \n",
    "client.annotation.post_annotation(table_name='test_table', data=[new_data])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. EMAnnotationSchemas\n",
    "\n",
    "The EMAnnotationSchemas client lets one look up the available schemas and how they are defined. This is mostly used for programmatic interactions between services, but can be useful when looking up schema definitions for new tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the list of schema \n",
    "One can get the list of all available schema with the `schema` method. Currently, new schema have to be generated on the server side, although we aim to have a generic set available to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['synapse',\n",
       " 'presynaptic_bouton_type',\n",
       " 'postsynaptic_compartment',\n",
       " 'microns_func_coreg',\n",
       " 'cell_type_local',\n",
       " 'flat_segmentation_reference',\n",
       " 'bound_tag',\n",
       " 'extended_classical_cell_type',\n",
       " 'plastic_synapse',\n",
       " 'glia_contact']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View a specific schema\n",
    "\n",
    "The details of each schema can be viewed with the `schema_definition` method, formatted as per JSONSchema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$ref': '#/definitions/FunctionalCoregistration',\n",
       " 'definitions': {'BoundSpatialPoint': {'properties': {'position': {'description': 'spatial position in voxels ofx,y,z of annotation',\n",
       "     'items': {'format': 'integer', 'title': 'position', 'type': 'number'},\n",
       "     'maxItems': 3,\n",
       "     'minItems': 3,\n",
       "     'postgis_geometry': 'POINTZ',\n",
       "     'title': 'position',\n",
       "     'type': 'array'},\n",
       "    'root_id': {'type': 'integer'},\n",
       "    'supervoxel_id': {'type': 'integer'}},\n",
       "   'required': ['position'],\n",
       "   'type': 'object'},\n",
       "  'FunctionalCoregistration': {'properties': {'func_id': {'description': 'functional cell ID',\n",
       "     'format': 'integer',\n",
       "     'title': 'func_id',\n",
       "     'type': 'number'},\n",
       "    'pt': {'$ref': '#/definitions/BoundSpatialPoint',\n",
       "     'description': 'location of cell body of functional cell',\n",
       "     'type': 'object'},\n",
       "    'type': {'description': 'type of annotation',\n",
       "     'drop_column': True,\n",
       "     'title': 'type',\n",
       "     'type': 'string'},\n",
       "    'valid': {'description': 'is this annotation valid',\n",
       "     'title': 'valid',\n",
       "     'type': 'boolean'}},\n",
       "   'required': ['func_id', 'pt', 'type'],\n",
       "   'type': 'object'}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_schema = client.schema.schema_definition('microns_func_coreg')\n",
    "example_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is mostly useful for programmatic interaction between services at the moment, but can also be used to inspect the expected form of an annotation by digging into the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'func_id': {'description': 'functional cell ID',\n",
       "   'format': 'integer',\n",
       "   'title': 'func_id',\n",
       "   'type': 'number'},\n",
       "  'pt': {'$ref': '#/definitions/BoundSpatialPoint',\n",
       "   'description': 'location of cell body of functional cell',\n",
       "   'type': 'object'},\n",
       "  'type': {'description': 'type of annotation',\n",
       "   'drop_column': True,\n",
       "   'title': 'type',\n",
       "   'type': 'string'},\n",
       "  'valid': {'description': 'is this annotation valid',\n",
       "   'title': 'valid',\n",
       "   'type': 'boolean'}},\n",
       " 'required': ['func_id', 'pt', 'type'],\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_schema['definitions']['FunctionalCoregistration']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“data_analysis”",
   "language": "python",
   "name": "jupyter_space"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
